import os
import numpy as np
import torch
from tqdm import tqdm  # Pasek postÄ™pu
from env import TradingEnv
from agent import DQNAgent
from utils import load_and_process_data, plot_results

# ğŸ“Š ZaÅ‚aduj dane rynkowe
prices, log_returns = load_and_process_data('data/etf_prices.csv')

# ğŸ¦ Inicjalizacja Å›rodowiska
env = TradingEnv(prices, log_returns)
state_size = env._get_state().shape[0]
action_size = env.n_assets  

# ğŸ”„ ÅšcieÅ¼ki do modelu i epsilon
MODEL_PATH = "dqn_trading_model.pth"
EPSILON_PATH = "epsilon.txt"

# ğŸ“Œ Tworzymy agenta
agent = DQNAgent(state_size, action_size)

# ğŸ”„ **Wczytujemy istniejÄ…cy model**
if os.path.exists(MODEL_PATH):
    agent.load_model(MODEL_PATH)
    print("âœ… Model wczytany!")
else:
    print("âš ï¸ Brak zapisanego modelu. Trening od zera.")

# ğŸ”„ **Wczytujemy epsilon, jeÅ›li istnieje**
if os.path.exists(EPSILON_PATH):
    with open(EPSILON_PATH, "r") as f:
        agent.epsilon = float(f.read().strip())
    print(f"ğŸ”„ Kontynuujemy trening z epsilon = {agent.epsilon:.4f}")
else:
    agent.epsilon = 1.0  # Pierwszy trening

# ğŸ“Œ Parametry treningowe
EPISODES = 2000
EPISODE_LENGTH = 252

# ğŸ“Š Zapis wynikÃ³w
all_rewards = []
all_portfolio_values = []
all_epsilons = []

# ğŸš€ Trening
progress_bar = tqdm(range(EPISODES), desc="ğŸ“ˆ Trening modelu", unit="epizod")

for episode in progress_bar:
    state = env.reset(episode_length=EPISODE_LENGTH)
    done = False
    total_reward = 0

    while not done:
        action = agent.act(state)
        next_state, reward, done = env.step(action)

        agent.remember(state, action, reward, next_state, done)
        state = next_state
        total_reward += reward

    agent.replay()  # Trening modelu

    # ğŸ“Š Zapis wynikÃ³w epizodu
    all_rewards.append(total_reward)
    all_portfolio_values.append(env._get_portfolio_value())
    all_epsilons.append(agent.epsilon)

    # ğŸ† Co 10 epizodÃ³w aktualizujemy target network
    if episode % 10 == 0:
        agent.update_target_network()

    # ğŸ”„ Aktualizacja paska postÄ™pu
    progress_bar.set_postfix({
        "ğŸ¯ Nagroda": f"{total_reward:.2f}",
        "ğŸ’° Portfel": f"{all_portfolio_values[-1]:.2f}",
        "ğŸ§  Epsilon": f"{agent.epsilon:.4f}"
    })

# ğŸ† Po treningu zapisujemy model i epsilon
agent.save_model(MODEL_PATH)

with open(EPSILON_PATH, "w") as f:
    f.write(str(agent.epsilon))

print(f"âœ… Model i epsilon zapisane!")

# ğŸ“Š Wizualizacja wynikÃ³w
plot_results(all_rewards, all_portfolio_values, all_epsilons, None)
